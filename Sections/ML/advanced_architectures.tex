\chapter{Advanced Architectures}\label{sec:advanced_architectures}

\section{Convolutional Nets}

\section{GoogLeNet}

\section{Autoencoders}

An autoencoder, shown in Figure~\ref{fig:autoencoder}, is an architecture which has the same number of neurons in its input and output layers, but which contains at least one hidden layer which has fewer neurons. These nets are trained in a semi-unsupervised manner, by simply trying to get the outputs to replicate the inputs. If we take $\hat{x}$ to be the input vector and $\hat{y}$ to be the output vector, a simple loss function for training an autoencoder is:

\begin{align}
    \mathcal{L} &= (\hat{y}-\hat{x})^2
\end{align}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Images/ML/autoencoder.png}
    \caption{Autoencoder architecture, from~\cite{autoencoderDiagram}.}
    \label{fig:autoencoder}
\end{figure}

The purpose of an autoencoder is to perform dimensionality reduction. In Figure~\ref{fig:autoencoder}, our net has learned a compressed encoding of its data space, as represented by the neurons on the $\hat{z}$ layer. The trained structure can then be used for many purposes, such as for data compression or for image generation with generative adversarial nets.

The layers of the autoencoder may be simply connected, or they can be more complex structures, such as convolutional layers.

\section{Generative Adversarial Nets}

A generative adversarial network (GAN) can be used to create simulated samples in some data space. A GAN can be used to generate images, speech, video, or any other kind of data. Two famous examples of this type of network are StyleGAN, which can be used to generate faces and other images, and Google DeepDream, which can be used to generate surreal pictures and videos.

In a GAN we have two nets, the generator and the discriminator, competing against each other. To explain the process we will consider the case of a GAN which can generate images of a requested class.

The generator begins as the decoder half of a trained autoencoder, as seen in Figure~\ref{fig:autoencoder}. This autoencoder has been trained not only to replicate input data, but also to perform classification on it, as represented by a subset of the neurons on the $\hat{z}$ layer. Thus, layer $\hat{z}$ is a compressed representation of the image space, with a subset of neurons indicating image class and the rest of the neurons representing other salient features about the image.

In order to generate a new image with the generator, we feed to layer $\hat{z}$ the desired classification, along with a vector of randomized noise for the other neurons. Propagating through the generator creates an output of the desired class.

The discriminator half of the network is a trained classifier which is able to determine the class of an image. We tweak this architecture by adding an output which also classifies the image as "real" or "generated".

This is where the "adversarial" part of a GAN comes into play. The two nets are now trained against each other. First we request an image of a random class from the generator, and feed that image to the discriminator. The generator loss function is based on the image classification accuracy of the discriminator and on whether or not the discriminator was fooled into thinking the image was real. Next we train the discriminator by feeding it a mix of real and generated images, and grading it by discrimination accuracy. This process is repeated back and forth until convergence.

\section{Recurrent Neural Nets}

Up until now we have described typical feedforward neural nets, where connections between nodes go from layer to layer and do not loop back to previous layers. We will now describe a class of recurrent architectures which do not exhibit this behavior. Rather, as seen in Figure~\ref{fig:RNN}, a recurrent neural net (RNN) has a set of "hidden" neurons which loop from the output layer back to the input layer. We can use these kinds of nets for processing time series or sequential data by feeding the data sequence (e.g. a string of words that form a sentence) into the net one piece at a time. We begin with a randomly initialized hidden state. After each piece of input passes through the net, we get an output and an updated hidden state. The hidden state is brought back around to the beginning of the net, and is fed back in along with the next piece of sequential data. In this way the hidden neurons are able to act as a sort of memory state, and to encode some sort of ongoing information about the sequence. A recurrent net can turn a sequence of inputs into a sequence of outputs, but in many situations we only care about the final output, which may be used for classification.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Images/ML/RNN.png}
    \caption{A standard recurrent neural net, shown in condensed form on the left, and in expanded form on the right. This net takes inputs $x_i$ and provides outputs $y_i$. A hidden state $h_i$ is retained in the net and passed back to the beginning after each input step.}
    \label{fig:RNN}
\end{figure}

The actual body of the RNN can be a neural net of any architecture, but in a standard RNN we typically use a simple few-layer densely connected net. The hidden state and output also typically pass through a $\tanh$ layer, with the output passing through an additional softmax layer. In this case, if we write the concatenation of $x_t$ and $h_{t-1}$ as $x_t \oplus h_{t-1}$ and the neural net weight matrix as $W$, we can represent the RNN via the following equations:

\begin{align}
    o_t\oplus h_t&=\tanh(W(x_t\oplus h_{t-1})) \\
    y_t&=\text{softmax}(o_t)
\end{align}

If we examine this equation, we can see a problem which commonly comes up when training an RNN via gradient descent. We see that the final output $y_n$ depends on $W$, $x_n$, and $h_{n-1}$. But then $h_{n-1}$ further depends on $x_{n-1}$ and $h_{n-2}$, and so on. If we work out the full derivative of $y_n$ with respect to $W$, we get the following:

\begin{align}
    \frac{dy_n}{dW}&=\frac{\partial y_n}{\partial W} + \frac{\partial y_n}{\partial h_{n-1}}\frac{dh_{n-1}}{dW} \\
    &=\frac{\partial y_n}{\partial W} + \frac{\partial y_n}{\partial h_{n-1}}[\frac{\partial h_{n-1}}{\partial W} + \frac{\partial h_{n-1}}{\partial h_{n-2}}\frac{dh_{n-2}}{dW}] \\
    &=\frac{\partial y_n}{\partial W} + \frac{\partial y_n}{\partial h_{n-1}}[\frac{\partial h_{n-1}}{\partial W} + \frac{\partial h_{n-1}}{\partial h_{n-2}}[
    \frac{\partial h_{n-2}}{\partial W} + \frac{\partial h_{n-2}}{\partial h_{n-3}}\frac{dh_{n-3}}{dW}
    ]] \\
    &=... \nonumber
\end{align}

We then see that for all $t$:

\begin{align}
    \frac{\partial h_t}{\partial h_{t-1}} &= \tanh'(W(x_t\oplus h_{t-1}))W \\
    \frac{\partial h_t}{\partial W} &= \tanh'(W(x_t\oplus h_{t-1}))(x_t\oplus h_{t-1}) \\
    \frac{dy_n}{dW} &\approx \sum_{t=1}^{n} [\tanh'(W(x_t\oplus h_{t-1}))]^{n+1-t}W^{n-t}(x_t\oplus h_{t-1})
\end{align}

The powers of $W$ and the powers of the derivative of $\tanh(x)$ are both problematic here, since they quickly approach zero, causing terms with low values of $t$ to essentially become irrelevant to training. In other words, the basic RNN has trouble learning long-range dependencies, and will in a sense "forget" the beginning of a sequence by the time it has calculated the final output. This behavior is called the vanishing gradient problem. In the case that W is large enough, we have the exploding gradient problem instead, but this issue is less common and is easily fixed by clipping gradients at a set maximum.

\section{LSTM and GRU}

One method of dealing with the vanishing gradient problem is the use of long short-term memory (LSTM) nets, first proposed in 1997. This type of net, shown in Figure~\ref{fig:LSTM_diagram}, has an internal structure in each recurrent cell which is more complex than a simple neural net. Each cell contains a forget gate, an input gate, and an output gate, which determine how much the cell state should update to accommodate new information, and how much information it should output. We have two internal states in an LSTM, $c_t$ and $h_t$. $c_t$ is similar to the hidden cell state in the basic RNN, and $h_t$ acts as the cell output at each step, but is also carried over to the input of the next step.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Images/ML/LSTM.png}
    \caption{LSTM architecture, showing the forget gate ($F_t$), input gate ($I_t$), and output gate ($O_t$) present in each cell. Taken from~\cite{LSTMDiagram}.}
    \label{fig:LSTM_diagram}
\end{figure}

The LSTM works as follows. At each time step $t$, we feed the input $x_t$, the previous output $h_{t-1}$, and the previous cell state $c_{t-1}$ into the net. $x_t$ and $h_{t-1}$ are used to determine the forget factor $F_t$, the input factor $I_t$, and the output factor $O_t$, all of which are vectors with components that lie between 0 and 1. The forget factor is multiplied onto $c_{t-1}$, allowing the cell to "forget" some of its previous state. The input factor is used to determine how much of the current input then gets added to the cell state. Finally, the output factor is multiplied by the new cell state to give $h_t$, the cell output. All together, the equations for an LSTM are as follows:

\begin{align}
    F_t &= \sigma(W_f (x_t \oplus h_{t-1})) \\
    I_t &= \sigma(W_i (x_t \oplus h_{t-1})) \\
    c_t &= c_{t-1} \cdot F_t + I_t \cdot\tanh(W_c (x_t \oplus h_{t-1})) \\
    O_t &= \sigma(W_o (x_t \oplus h_{t-1})) \\
    h_t &= O_t \cdot\tanh(W_h c_t)
\end{align}

The derivation is beyond the scope of this overview, but the gradient of the loss in this case will not have an exponential dependence on any weight matrix $W$, nor on the derivative of the sigmoid or tanh. Essentially, the $c_t$ state ensures that all inputs have non-zero contribution to the loss term. Thus, the LSTM is much less prone to the vanishing gradient problem.

The gated recurrent unit (GRU) net, introduced in 2014 and shown in Figure~\ref{fig:GRU_diagram}, further improves on the LSTM by reducing its complexity while maintaining its avoidance of the vanishing gradient problem. A GRU only has two gates, an update gate and a reset gate. In this architecture the reset gate controls how much information from the hidden state gets added to the input. The update gate conversely controls how much input information gets mixed back into the hidden state. The hidden state $h_t$ also acts as the output at each step, making the structure of the GRU somewhat less effective for multi-output data.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Images/ML/GRU.png}
    \caption{GRU architecture, showing the update gate ($Z_t$) and reset gate ($R_t$) present in each cell. Taken from~\cite{GRUDiagram}.}
    \label{fig:GRU_diagram}
\end{figure}

\begin{align}
    R_t &= \sigma(W_r (x_t \oplus h_{t-1})) \\
    Z_t &= \sigma(W_z (x_t \oplus h_{t-1})) \\
    h_t &= Z_t h_{t-1} + (1-Z_t)\tanh(R_t h_{t-1} + x_t)
\end{align}

\section{Seq2Seq}

A seq2seq architecture consists of an encoder recurrent net and a decoder recurrent net, as seen in Figure~\ref{fig:seq2seq}. This type of net takes an input sequence and processes it with the encoder. This processed vector, which contains information about the entire sequence, is then passed to the decoder and used to output a different sequence, not necessarily of the same length as the input.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{Images/ML/seq2seq.png}
    \caption{Seq2Seq architecture. The encoder takes words in a sentence, embeds them in a word vector space, and passes the embedded vectors into an RNN. The final output of the encoder is sent to the decoder as its initial hidden state. The decoder begins with SOS as its initial input, and loops until it outputs EOS. The entire decoder sequence is the output sentence.}
    \label{fig:seq2seq}
\end{figure}

Seq2Seq is commonly used for language translation, so inputs in these cases are first transformed into a "word vector" in some embedding space before they go through the encoder. Describing word embeddings is outside the scope of this thesis, but the idea is that similar words are close together in this vector space, and pairs of words with the same relations have similar vector differences. In other words, the embedding captures some semantic structure between words. Common embeddings include word2vec and GloVe.

In more detail, the encoder takes a sequence of inputs $\hat{x}$ and runs it through an RNN to produce a sequence of outputs $\hat{s}$. In a simple seq2seq architecture, we take only the final output, which we denote as $s$, to pass to the decoder. The decoder takes $s$ as an initial hidden state, and the start-of-string (SOS) signal as an initial input, and performs a typical recurrent architecture loop to generate an output sequence. The sequence ends when the decoder outputs an end-of-string (EOS) signal.

A seq2seq architecture may also include an attention mechanism, as shown in Figure~\ref{fig:seq2seq_with_attention}. In this case the decoder has an additional step where it uses its input and hidden state to obtain an attention vector. We take the dot product of this attention vector with the entire encoder output sequence $\hat{s}$ to get a sort of weighted vector. Essentially, the attention vector allows us to focus on different parts of the input sequence as we're producing the output.

Details about the training of seq2seq are outside the scope of this thesis.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.55\linewidth]{Images/ML/seq2seq_with_attention.png}
    \caption{Seq2Seq architecture with an attention mechanism added. Now the entire encoder output is used by the decoder. The final output of the encoder is still used as the decoder's initial hidden state, but now the hidden state and decoder input are used to calculate an attention vector, which passes through a softmax layer to add up to 1. The dot product of this attention vector with the entire encoder output sequence then produces a weighted encoder output which is sent to the decoder RNN.}
    \label{fig:seq2seq_with_attention}
\end{figure}

\section{Transformers}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\linewidth]{Images/ML/transformer.png}
    \caption{<transformer>. Taken from~\cite{transformer}.}
    \label{fig:transformer}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{Images/ML/transformer_dot_product_attention.png}
    \caption{<self attention dot product>. Taken from~\cite{transformer}.}
    \label{fig:transformer_dot_product_attention}
\end{figure}

\section{Deep Sets}

\section {Set Transformers}